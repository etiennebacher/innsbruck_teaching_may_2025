---
title: "Webscraping -- Part 1"
subtitle: "Static webscraping"

format:
 revealjs:
   incremental: false
   theme: [moon, custom.scss]
   pdf-separate-fragments: true
   strip-comments: true
   highlight-style: atom-one
   auto-animate-duration: 0.8
   code-copy: true
   slide-number: true

execute:
  eval: false
  echo: true
---

<!-- Print to PDF -->
<!-- Follow this: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf -->
<!-- Use chrome and not firefox -->


## Before we start

This course:

- 3 parts: webscraping (parts 1 and 2), and handling large data with R and Python
- No assignments, only check for attendance
- All slides:
  - [https://innsbruck-webscraping-part-1.etiennebacher.com](https://innsbruck-webscraping-part-1.etiennebacher.com)
  - [https://innsbruck-webscraping-part-2.etiennebacher.com](https://innsbruck-webscraping-part-2.etiennebacher.com)
  - [https://innsbruck-large-data-r-python.etiennebacher.com](https://innsbruck-large-data-r-python.etiennebacher.com)

<!-- About me: -->

<!-- - Defended my PhD last November -->
<!-- - Worked on migration economics (and some economic history), will be around during the week -->
<!-- - Contact: etienne.bacher@liser.lu -->


## Plan for today 

* Introduction:
  - Using APIs and webscraping
  - How is a webpage built?
  - Q&A number 1

. . .

* Hands-on practice: scraping El Pais for electoral results
  - Scraping one page
  - Being polite
  - Scraping hundreds of pages
  - Q&A number 2

## Introduction

**What is webscraping?**

<br>

It is the action of collecting data from webpages.

*Anything that is visible on a webpage can be webscraped.*

. . .

<br>

**Why is it useful?**

<br>

To get original data that isn't available anywhere else.

## Introduction

**But** webscraping can be time-consuming and isn't always possible.

<br>

Order of preference:

* online service has an API (next slide)
* ask the online service
* scrape
  - static pages (today)
  - dynamic pages (tomorrow)
  
## Using an API

The objective of some websites is to provide data to people: they *want* us to being able to access this data easily.

<br>

. . .

They usually provide an API (Application Programming Interface) so that we can interact with their services via code.

<br>

This is **better than webscraping**.
  
## Using an API

Example: [World Development Indicators API](https://datahelpdesk.worldbank.org/knowledgebase/articles/889392-about-the-indicators-api-documentation)

<br>

In R, we can use the package `WDI` to access this API:

```{r, eval = FALSE}
WDI(
   indicator = 'NY.GDP.PCAP.KD', 
   country = c('MX','CA','US'), 
   start = 1960, 
   end = 2012
)
```

## Using an API

Many data services have an API: World Bank, Eurostat, Google Maps, ArcGIS, US Census, etc.

<br>

Sometimes, those services require an API key (might need to pay for this).

<br>

If you want data from a source, try searching "R package API \<name of the service\>" first.


## Objectives

For this morning, the objectives are to:

* understand the content of a webpage
* scrape a webpage
* be able to apply this to thousands of pages
* process the scraped content

# What does a webpage contain?

## What does a webpage contain?

The web works with 3 languages:

1.   HTML: content and structure of the page
1.   CSS: style of the page
1.   JavaScript: interactions with the page


## HTML

HTML tells us the content of the page and how it is structured using **tags**.

<br>
Example: `<p>Hello my name is <b>Etienne</b></p>`

<br>

[\<p\>]{style="color: beige;"} is a *paragraph* tag and [\<b\>]{style="color: beige;"} is a *bold* tag.

## HTML

Popular tags:

- `h1`-`h4`: headers
- `p`: paragraph
- `a`: link
- `div`: generic container

## HTML

A tag can have *attributes*, e.g. `class` or `style`:

<br>

`<p class = "my_class">Hello my name is <b>Etienne</b></p>`

<br>
This is used either for custom styling with CSS or for interactions with JavaScript.

## HTML

How can we see the HTML of a page?

<br>

Either:

- right-click on a page and "Inspect"
- Ctrl (or Cmd) + Shift + C

## HTML

<div style="text-align: center">
<img src="img/console.png" style="width: 70%">
</div>

## CSS

CSS is a language only about the **style** of the page.

<br>

There is no content at all, e.g. it's only here to say "bold text should be in red and underlined".

<br>

Example:

```
b {
  color: red;
  border-bottom: 1px dashed;
}
```
## CSS

It is still useful to know *CSS paths* because we'll use them to select specific elements in the page.

. . .

```
p             : all "p" tags
p span        : all "span" contained in "p" tags
p, a          : all "p" and "a" tags
#id1          : all elements with id equal to id1
.class1       : all elements of class "class1"
p.class1      : all "p" elements of class "class1"
p.class1 span : all "span" in "p" tags of class "class1"
p > span      : all "span" that are direct children of p
h1 + p        : all "p" that follow *directly* an "h1" (direct sibling)
h1 ~ p        : all "p" that follow an "h1" (siblings placed after)
[id]          : all elements with an existing "id" attribute
[class^=my]   : all elements whose class starts with "my"
p[class*=low] : all "p" elements whose class contains the string low
```

## Example

<br>

```{.html}
<h2>Who am I?</h2>
<div class="intro">
  <p>I'm <span class="age">34</span> and 
    measure <span class="unit">1.70m</span>.</p>
</div>
<div class="info">
  <div>
    <p id="like">What I like:</p>
    <ul>
      <li>Barcelona</li>
      <li>winning the Ballon d'Or every odd year</li>
    </ul>
    <p class="extra">I forgot to say that I like scoring over 100 
       goals per season.</p>
  </div>
</div>
```

## Example

:::: {.columns}
::: {.column width="60%"}
CSS selector for this paragraph?
:::
::: {.column width="40%" .fragment .fade-in}
`p.extra`
:::
::::

```{.html code-line-numbers="13-14"}
<h2>Who am I?</h2>
<div class="intro">
  <p>I'm <span class="age">34</span> and 
    measure <span class="unit">1.70m</span>.</p>
</div>
<div class="info">
  <div>
    <p id="like">What I like:</p>
    <ul>
      <li>Barcelona</li>
      <li>winning the Ballon d'Or every odd year</li>
    </ul>
    <p class="extra">I forgot to say that I like scoring over 100 
       goals per season.</p>
  </div>
</div>
```

## Example

:::: {.columns}
::: {.column width="60%"}
CSS selector for this `div`?
:::
::: {.column width="40%" .fragment .fade-in}
`div.info > div`
:::
::::

```{.html code-line-numbers="7-15"}
<h2>Who am I?</h2>
<div class="intro">
  <p>I'm <span class="age">34</span> and 
    measure <span class="unit">1.70m</span>.</p>
</div>
<div class="info">
  <div>
    <p id="like">What I like:</p>
    <ul>
      <li>Barcelona</li>
      <li>winning the Ballon d'Or every odd year</li>
    </ul>
    <p class="extra">I forgot to say that I like scoring over 100 
       goals per season.</p>
  </div>
</div>
```

## JavaScript

JavaScript (JS) is a language that takes care of all the interactions with the page:

* add a popup when we scroll on the page
* show a notification when a button is pressed
* etc.

<br>

It makes it more difficult to scrape, we'll handle that in part 2 tomorrow.

---

<br>

Today, we only look at HTML and CSS and focus on static webpages.

<br>

**Static webpage**:

-   all the information is loaded with the page;
-   changing a parameter modifies the URL

Examples: [Wikipedia](https://en.wikipedia.org/wiki/Web_scraping){.external target="_blank"}, [IMDB](https://www.imdb.com/name/nm0001392/?ref_=nv_sr_srsg_0){.external target="_blank"}.

<br>

**Dynamic webpage**: the website uses JavaScript to fetch data from
their server and *dynamically* update the page.

Example: [Premier League stats](https://www.premierleague.com/stats/top/players/goal_assist){.external target="_blank"}.

# Q&A number 1

# Case study: electoral results in Spain

## Case study: electoral results in Spain

<br>

Get results of municipal elections in Spain for 2019 from [El Pais](https://resultados.elpais.com/elecciones/2019/municipales/01/04/19.html).

<br>

Turns out that since I've done this webscraping, an R package became available: [`infoelectoral`](https://ropenspain.github.io/infoelectoral/).

<br> 

Let's pretend there isn't any.

## Case study: electoral results in Spain

My process for webscraping is always the same:

1. focus on a single page
1. extract data for this page
1. generalize to other pages
1. add error handling and logging

. . .

Importantly, *separate webscraping from data processing* (we'll see why later).

<br>

{{< fa arrow-right >}} Let's start with the results in [Bacares in 2019](https://resultados.elpais.com/elecciones/2019/municipales/01/04/19.html).



## Case study: electoral results in Spain

In R, the main package for webscraping is [`rvest`](https://rvest.tidyverse.org/).

<br>

We can read HTML with `read_html()` and then select specific HTML elements with `html_element()` or `html_elements()` (this is where our CSS knowledge is useful).


## Case study: electoral results in Spain

<br>

*Part 1 in R*

## Being polite

Three rules to follow for polite webscraping:

- seeking permission
- taking slowly
- never asking twice

. . .

<br>

There's an R package for that<sup>TM</sup>: [`polite`](https://dmi3kno.github.io/polite/).

## Being polite

Instead of:

```{r}
url <- "https://www.something.html"
read_html(url)
```

we do:

```{r}
library(polite)
url <- bow("https://www.something.html")
scrape(url)
```

## Being polite

<br>

By default we use a 5-second delay or what the website recommends.

<br>

If we want to lower this, we need to specify our own "User-Agent", which is sort of our ID card on the web.

## Case study: electoral results in Spain

Now we have the table for one municipality: how can we generalize that (politely)?

. . .

<br>

The URL is:

```
https://resultados.elpais.com/elecciones/2019/municipales/01/04/19.html
```

. . .

Three components: 

* 01 is the comunidad code (for El Pais)
* 04 is the province code
* 19 is the municipality code

## Case study: electoral results in Spain

To generalize the code, we need to loop through all of those combinations:

* for each comunidad:
   * get the list of provinces
   * for each province:
      * get the list of municipalities
      * for each municipality:
         * get the data
         
<br>

. . .

{{< fa arrow-right >}} new objective: gather all those combinations from the website

## Case study: electoral results in Spain

*Part 2 in R*

## Case study: electoral results in Spain

<br><br>

Great, let's run everything!

<br>

. . .

{{< fa arrow-right >}} &nbsp; Not so fast



# Error handling

## Catching errors

The default behavior of errors is to stop the script, which leads to this kind
of situation:

. . .

* 7pm - "great, I can just run the code during the night and go home"

. . .

* 7:02pm - *error in the code*

. . .

* 8am - "Fu@$?!"

. . .

<br>

{{< fa arrow-right >}} &nbsp; need to handle errors with `tryCatch()`



## Catching errors

<br> <br> 

<p> <span style="color: #ffaa00">try</span><span style="color: #d24dff">Catch</span> :</p>

* <p><span style="color: #ffaa00">try</span> to run an expression </p>
* <p> <span style="color: #d24dff">catch</span> the potential warnings/errors </p>

## Catching errors

Example: try to compute `log("a")`.

```{r, eval=TRUE, error=TRUE}
log("a")
```

<br>

. . .

What if I want to return `NA` instead of an error?


## Catching errors

```{.r}
tryCatch(
  # try to evaluate the expression
  {
    <EXPRESSION>,
  },

  # what happens if there's a warning?
  warning = function(w) {
    <BEHAVIOR WHEN THERE IS A WARNING>
  },
  # what happens if there's an error?
  error = function(e) {
    <BEHAVIOR WHEN THERE IS AN ERROR>
  }
)
```

## Catching errors

```{r}
tryCatch(
  # try to evaluate the expression
  {
    log("a")
  },

  # what happens if there's a warning?
  warning = function(w) {
    print("There was a warning. Here's the message:")
    print(w)
  },

  # what happens if there's an error?
  error = function(e) {
    print("There was an error. Returning `NA`.")
    return(NA)
  }
)
```

## Catching errors {.nobreak}

```{r, eval=TRUE, error=TRUE}
tryCatch(
  # try to evaluate the expression
  {
    log("a")
  },

  # what happens if there's a warning?
  warning = function(w) {
    print("There was a warning. Here's the message:")
    print(w)
  },

  # what happens if there's an error?
  error = function(e) {
    print("There was an error. Returning `NA`.")
    return(NA)
  }
)
```


## Example with a loop

Create a fake loop that goes from `i = 1:10` and creates a list containing `i*2`.
Let's say that there's an error when `i = 3`:

```{r, eval=FALSE, error=TRUE}
x <- list()
for (i in 1:10) {

  if (i == 3) {
    stop("This is an error.")  # intentional error
  } else {
    x[[i]] <- 2*i
  }

  print(paste0("i = ", i, ". So far so good."))
}
```

## Example with a loop

Create a fake loop that goes from `i = 1:10` and creates a list containing `i*2`.
Let's say that there's an error when `i = 3`:

```{r, eval=TRUE, error=TRUE}
x <- list()
for (i in 1:10) {

  if (i == 3) {
    stop("This is an error.")  # intentional error
  } else {
    x[[i]] <- 2*i
  }

  print(paste0("i = ", i, ". So far so good."))
}
```



## Example with a loop

<br>

```{r, eval=TRUE, error=TRUE}
print(x)
```

<br>

{{< fa arrow-right >}} &nbsp; We don't have values for `i >= 3` because there was
an error that stopped the loop.


## Catching the error

Now, let's catch the error to avoid breaking the loop:

```{.r code-line-numbers="4-16"}
x <- list()
for (i in 1:10) {
  
  tryCatch(
    {
      if (i == 3) {
        stop("This is an error.")  # intentional error
      } else {
        x[[i]] <- 2*i
      }
    },
    error = function(e) {
      print(paste0("Error for i = ", i, ". `x[[", i, "]]` will be NULL."))
      x[[i]] <- NULL
    }
  )
  
  print(paste0("i = ", i, ". So far so good."))
}
```



## Catching the error

```{r, eval=TRUE, echo=FALSE, error=TRUE}
x <- list()
for (i in 1:10) {
  
  tryCatch(
    {
      if (i == 3) {
        stop("This is an error.")  # intentional error
      } else {
        x[[i]] <- 2*i
      }
    },
    error = function(e) {
      print(paste0("Error for i = ", i, ". `x[[", i, "]]` will be NULL."))
      x[[i]] <- NULL
    }
  )
  
  print(paste0("i = ", i, ". So far so good."))
}

print(x)
```

## Using `tryCatch` in our loop

*Part 3 in R*

## Processing the data

Note that until now, we have only *gathered* the data, not *processed* it.

<br>

. . .

Maybe we even did too much work!

<br>

We could have simply downloaded each HTML page locally: remember that a webpage is just a text file!

## Processing the data

Advantages of downloading locally:

* we reduce the time where we need an internet connection, therefore reducing several risks of failure
* we improve the reproducibility of the dataset

Cons:

* depending on the number of pages and their size, it can take some storage space (but only if you really download tens or hundreds of thousands)


## Ethics


Pay attention to a website's Terms of Use/Service.

<br>

Some websites explicitly say that you are not allowed to programmatically access
their resources.

<br>

![](img/terms-of-use.png)


## Legality


**I'm not a lawyer.**

<br>

For me, scraping politely (slowly, identifying yourself) on websites where data is publicly available is fine.

<br>

Scraping data behind a paywall or needing an account is not, because the data is not publicly available. 

<br>

Can get banned temporarily or permanently from some websites.


## Conclusion

<br>

Sometimes, scraping static websites is needed (but check for available APIs first!).

<br>

The package `rvest` is the best tool to do that in R. If you prefer Python, use `beautifulsoup`.


## Conclusion

<br>

Code on GitHub (not available yet): [https://github.com/etiennebacher/innsbruck_teaching_may_2025](https://github.com/etiennebacher/innsbruck_teaching_may_2025)

<br>

Acknowledgements: some of those slides were adapted from [Laurent Bergé's slides](https://shimmering-maamoul-4f92fd.netlify.app/docs/2022-2023_m2-data-2_webscraping-1_intro).



## Session information

<br>

```{r echo = FALSE, eval = TRUE}
sessioninfo::session_info()
```

