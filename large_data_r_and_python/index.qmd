---
title: "Handling large data with polars and tidypolars"

format:
 revealjs:
   incremental: false
   theme: [moon, custom.scss]
   pdf-separate-fragments: true
   strip-comments: true
   highlight-style: github
   auto-animate-duration: 0.8
   code-copy: true
   slide-number: true

execute:
  eval: true
  echo: true
---

## Plan for today {.nobreak}

* Introduction:
  - What is Polars and why is it useful?
  - Broad presentation of the reasons it is fast
  - Q&A number 1

. . .

* Hands-on practice: IPUMS census data (samples)
  - Goal: install and load polars, load data, perform easy operations and write it back to another file
  - Using expressions
  - Q&A number 2

. . .

* Going further
  - Using streaming, plugins, and future extensions
  - Q&A number 3

## What this session will *not* cover

Statistical modelling with big data.

<br>

The objective is more to see how we can start from very fine-grained data (full-count census, mobile phone data at the minute level) to some smaller, aggregated datasets that we can use for regressions.

<br>

Note that some statistical packages can still perform very well on millions of observations, like `fixest` in R.


## Motivation

<br><br>

The use of big data in all disciplines is growing:

* full censuses
* genomics data
* mobile phone data
* and more...


## Motivation

<br><br>

Usually, we load datasets directly in R (or Stata, or Python, or any other tool).

<br>

This means that **we are limited by the amount of RAM**^[Random Access Memory, where the data can be accessed by the programs.] on our computer.

<br>

You want to handle more data? Get more RAM.


## Motivation

<br><br>

But we can't necessarily have all the RAM we need.

. . .

<br><br>

And we don't necessarily *need* all this RAM in the first place.

# Introduction to Polars

## Introduction

<br>

`polars` is a recent DataFrame library that is available for several languages:

* Python
* R
* Rust
* and more

<br>

. . .

Built to be very fast and memory-efficient thanks to several mechanisms.

<br>

Very consistent and readable syntax.

# Eager vs Lazy

## Eager vs Lazy

**Eager evaluation:** all operations are run line by line, in order,
and directly applied on the data. This is the way we're used to.

. . .

```{r, eval=FALSE}
### Eager

# Get the data...
my_data = eager_data
  # ... and then sort by iso...
  .sort(pl.col("iso"))
  # ... and then keep only Japan...
  .filter(pl.col("country") == "Japan")
  # ... and then compute GDP per cap.
  .with_columns(gdp_per_cap = pl.col("gdp") / pl.col("pop"))

# => get the output
```

## Eager vs Lazy

**Lazy evaluation:** operations are only run when we call a specific
function at the end of the chain, usually called `collect()`.

. . .

```{r, eval=FALSE}
### Lazy

# Get the data...
my_data = lazy_data
  # ... and then sort by iso...
  .sort(pl.col("iso"))
  # ... and then keep only Japan...
  .filter(pl.col("country") == "Japan")
  # ... and then compute GDP per cap.
  .with_columns(gdp_per_cap = pl.col("gdp") / pl.col("pop"))

# => you don't get results yet!

my_data.collect() # this is how to get results
```


## Eager vs Lazy

<br>

When dealing with large data, it is better to use lazy evaluation:

<br>

1. Optimize the code
2. Catch errors before computations
3. Use streaming mode


## 1. Optimize the code

<br>

The code below takes some data, sorts it by a variable, and then
filters it based on a condition:

<br>

```{r, eval=FALSE}
data
  .sort(pl.col("country"))
  .filter(pl.col("country").is_in(["United Kingdom", "Japan", "Vietnam"]))
```

<br>

Do you see what could be improved here?


## 1. Optimize the code

<br>

The problem lies in the order of operations: **sorting data is much slower than filtering it**.

. . .

<br>

Let's test with a dataset of 50M observations and 10 columns:

```{r echo=FALSE}
library(dplyr, warn.conflicts = FALSE)
values <- rnorm(50 * 1e6)
data <- data.frame(
  country = rep_len(
    countrycode::codelist$country.name.en,
    length.out = 50 * 1e6
  ),
  year = sample(1960:2020, 50 * 1e6, TRUE),
  var1 = values,
  var2 = values,
  var3 = values,
  var4 = values,
  var5 = values,
  var6 = values,
  var7 = values,
  var8 = values
)
```

:::: {.columns}

::: {.column width="50%"}

```{python, eval=FALSE}
data
  .sort(pl.col("country"))
  .filter(pl.col("country").is_in(["United Kingdom", "Japan", "Vietnam"]))
```
```{r timing1, echo=FALSE, eval=TRUE, cache=TRUE}
system.time({
  data |>
    arrange(country) |>
    filter(country %in% c("United Kingdom", "Japan", "Vietnam"))
})
```

:::

::: {.column width="50%"}
```{python, eval=FALSE}
data
  .filter(pl.col("country").is_in(["United Kingdom", "Japan", "Vietnam"]))
  .sort(pl.col("country"))
```
```{r timing2, echo=FALSE, eval=TRUE, cache=TRUE}
system.time({
  data |>
    filter(country %in% c("United Kingdom", "Japan", "Vietnam")) |>
    arrange(country)
})
```

:::

::::



## 1. Optimize the query plan

<br><br>

There is probably tons of suboptimal code in our scripts.

. . .

<br>

But it's already hard enough to make scripts that work and that are
reproducible, **we don't want to spend even more time trying to
optimize them**.

. . .

<br>

{{< fa arrow-right >}} &nbsp; Let `polars` do this automatically by using lazy data.


## 1. Optimize the query plan

<br>

When we call `collect()`, `polars` doesn't directly execute the code. Before that, it does a lot of optimizations to be sure that we don't do inefficient operations.

. . .

<br>

Examples of optimizations:

* do not load rows that are filtered out in the query;
* do not load variables (columns) that are never used;
* cache and reuse computations
* and many more things


## 1. Optimize the query plan {.nobreak}

<br>

Workflow:

1. *scan* the data to get it in *lazy* mode.

::: {.panel-tabset}

## Python

```python
import polars as pl

raw_data = pl.scan_parquet("path/to/file.parquet")

# Or
pl.scan_csv("path/to/file.csv")
pl.scan_json("path/to/file.json")
...
```
## R (polars)

```r
library(polars)

raw_data = pl$scan_parquet("path/to/file.parquet")

# Or
pl$scan_csv("path/to/file.csv")
pl$scan_json("path/to/file.json")
...
```
## R (tidypolars)

```r
library(tidypolars)

raw_data = scan_parquet_polars("path/to/file.parquet")

# Or
scan_csv_polars("path/to/file.csv")
scan_json_polars("path/to/file.json")
...
```
:::


This only returns the schema of the data: the column names and their types (character, integers, dates, ...).

## 1. Optimize the query plan {.nobreak}

<br>

Workflow:

2. Write the code that you want to run on the data: filter, sort, create new variables, etc.

::: {.panel-tabset}

## Python

```python
my_data = raw_data
   .sort("iso")
   .filter(
      pl.col("gdp") > 123,
      pl.col("country").is_in(["United Kingdom", "Japan", "Vietnam"])
   )
   .with_columns(gdp_per_cap = pl.col("gdp") / pl.col("population"))
```

## R (polars)

```r
my_data = raw_data$
   sort("iso")$
   filter(
      pl$col("gdp") > 123,
      pl$col("country")$is_in(c("United Kingdom", "Japan", "Vietnam"))
   )$
   with_columns(gdp_per_cap = pl$col("gdp") / pl$col("population"))
```

## R (tidypolars)

```r
my_data = raw_data |>
   arrange(iso) |>
   filter(
      gdp > 123,
      country %in% c("United Kingdom", "Japan", "Vietnam")
   ) |>
   mutate(gdp_per_cap =  gdp / population)
```
:::


## 1. Optimize the code {.nobreak #optimize-plan}

<br>

Workflow:

<br>

3. Call `collect()` at the end of the code to execute it.

::: {.panel-tabset}

## Python

```python
my_data.collect()
```

## R (polars)

```r
my_data$collect()
```

## R (tidypolars)

```r
my_data |> compute()
```
:::


## 1. Optimize the query plan

:::{.callout-tip}
You can see the "query plan" (i.e. all the steps in the code) as it was originally written with `my_data.explain(optimized=False)`.

<br>

The query plan that is **actually run by Polars** can be seen with `my_data.explain()`.

:::

# 2. Catch errors before computations

## 2. Catch errors before computations

Calling `collect()` doesn't start computations right away.

<br>

First, `polars` scans the code to ensure there are no schema errors, i.e. check
that we don't do "forbidden" operations.

<br>

For instance, doing `pl.col("gdp") > "France"` would be an error: we can't compare
a number to a character.

<br>

In this case, that would return:

```
polars.exceptions.ComputeError: cannot compare string with numeric data
```



## 3. Use streaming mode

See in the last part.


# Q&A number 1


# Hands-on practice: IPUMS census data (samples)

## A note on file formats

We're used to a few file formats: CSV, Excel, `.dta`. Polars can read
most of them (`.dta` is not possible for now).

. . .

<br>

When possible, use the **Parquet format** (`.parquet`).

<br>

Pros:

* large file compression
* stores statistics on the columns (e.g. useful for filters)

Cons:

* might be harder to share with others (e.g. Stata doesn't have an easy way to read/write Parquet)


## Setting up Python

Keeping a clean Python setup is notoriously hard:

![](img/xkcd_1987.png)

## Data to use

* IPUMS 1% (or 5% in some cases) sample for 1850-1960

<br>

* About 23M observations, 116 variables

<br>

* See the WeTransfer link I sent you

## Objectives

* Python
  - set up the python project and environment
  - install and load polars
  - scan the data and read a subset
  - perform a simple filter
  - perform an aggregation
  - chaining expressions
  - exporting data

* Same with R (both `polars` and `tidypolars`)


# Going further

## Larger-than-memory data

Sometimes, data is just too big for our computer, even after all optimizations.

<br>

In this case, `collect()`ing the data will crash the Python or R session.

<br>

. . .

What are possible strategies for this?


## Larger-than-memory data

1. Use **streaming mode**

. . .

**Streaming** is a way to run the code on data by batches to avoid using all memory
at the same time.

. . .

<br>

Polars takes care of splitting the data and runs the code piece by piece.

## Larger-than-memory data

Using this is extremely simple: instead of calling `collect()`, we
call `collect(engine = "streaming")`.

<br>

:::{.callout-warning}
Some operations might be unavailable in streaming mode but this number should be quite low and become lower.
:::

## Larger-than-memory data

2. Think more about **whether you actually need** the data in the session

<br>

Maybe you just want to write the data to another file.

. . .

Instead of doing `collect()` + `write_*()`, use `sink_*` functions (e.g. `sink_parquet()`).

<br>

This will run the query and write the results progressively to the output path, without collecting in the session.

## Plugins

*Python only* (for now)

<br>

Polars accepts extensions taking the form of new expressions namespaces.

<br>

Just like we have `.str.split()` for instance, we could have `.dist.jarowinkler()`.

<br>

List of Polars plugins: [https://github.com/ddotta/awesome-polars#polars-plugins](https://github.com/ddotta/awesome-polars#polars-plugins)

## GIS and Polars

While there is some demand for a `geopolars` that would enable GIS operations in Polars DataFrame or LazyFrame, this doesn't exist for now.

<br>

Groundwork for this should start in the coming months so you might expect some movement in `geopolars` in 2026.

## Going further

<br><br>

If you're interested:

* [Polars official documentation](https://docs.pola.rs/)

* [R-Polars documentation](https://pola-rs.github.io/r-polars/)

* [Python `tidypolars`](https://github.com/markfairbanks/tidypolars)

* [R `tidypolars`](https://github.com/etiennebacher/tidypolars)

* [(Pretty) big data wrangling with DuckDB and Polars](https://grantmcdermott.com/duckdb-polars/slides/slides.html)



# Conclusion

## Conclusion

<br><br>

Getting bigger computers shouldn't necessarily be the first reflex to handle
large data.

<br>

There are several other tools available: `arrow`, `DuckDB`, `Spark`, and others.

<br>

Do you use any of them? How do you deal with large datasets?

## {.nobreak}

<br><br>
<br><br>

Slides: [https://github.com/etiennebacher/handling-large-data](https://github.com/etiennebacher/handling-large-data)

<br><br>

Typos, comments: [https://github.com/etiennebacher/handling-large-data/issues](https://github.com/etiennebacher/handling-large-data/issues)
