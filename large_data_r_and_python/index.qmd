---
title: "Handling large data with polars and tidypolars"

format:
 revealjs:
   incremental: false
   theme: [moon, custom.scss]
   pdf-separate-fragments: true
   strip-comments: true
   highlight-style: github
   auto-animate-duration: 0.8
   code-copy: true
   slide-number: true

execute:
  eval: true
  echo: true
---

## Plan for today {.nobreak}

* Introduction:
  - What is Polars and why is it useful?
  - Broad presentation of the reasons it is fast
  - Q&A number 1

. . .

* Hands-on practice: UK census data
  - Goal: install and load polars, load data, perform easy operations and write it back to another file
  - Using expressions
  - Q&A number 2

. . .

* Going further
  - Using streaming, plugins, and future extensions
  - Q&A number 3


## Motivation

<br><br>

The use of big data in all disciplines is growing:

* full censuses
* genomics data
* mobile phone data
* and more...


## Motivation

<br><br>

Usually, we load datasets directly in R (or Stata, or Python, or any other tool).

<br>

This means that **we are limited by the amount of RAM**^[Random Access Memory, where the data can be accessed by the programs.] on our computer.

<br>

You want to handle more data? Get more RAM.


## Motivation

<br><br>

But we can't necessarily have all the RAM we need.

. . .

<br><br>

And we don't necessarily *need* all this RAM in the first place.

# Introduction to Polars

## Introduction

<br>

`polars` is a recent DataFrame library that is available for several languages:

* Python
* R
* Rust
* and more

<br>

. . .

Built to be very fast and memory-efficient thanks to several mechanisms.

<br>

Very consistent and readable syntax.

# Eager vs Lazy

## Eager vs Lazy

**Eager evaluation:** all operations are run line by line, in order,
and directly applied on the data. This is the way we're used to.

. . .

```{python, eval=FALSE}
### Eager

# Get the data...
my_data = eager_data
  # ... and then sort by iso...
  .sort(pl.col("iso"))
  # ... and then keep only Japan...
  .filter(pl.col("country") == "Japan")
  # ... and then compute GDP per cap.
  .with_columns(gdp_per_cap = pl.col("gdp") / pl.col("gdp"))

# => get the output
```

## Eager vs Lazy

**Lazy evaluation:** operations are only run when we call a specific
function at the end of the chain, usually called `collect()`.

. . .

```{python, eval=FALSE}
### Lazy

# Get the data...
my_data = lazy_data
  # ... and then sort by iso...
  .sort(pl.col("iso"))
  # ... and then keep only Japan...
  .filter(pl.col("country") == "Japan")
  # ... and then compute GDP per cap.
  .with_columns(gdp_per_cap = pl.col("gdp") / pl.col("gdp"))

# => you don't get results yet!

my_data.collect() # this is how to get results
```


## Eager vs Lazy

<br>

When dealing with large data, it is better to use lazy evaluation:

<br>

1. Optimize the code
2. Catch errors before computations
3. Use streaming mode


## 1. Optimize the code

<br>

The code below takes some data, sorts it by a variable, and then
filters it based on a condition:

<br>

```{r, eval=FALSE}
data
  .sort(pl.col("country"))
  .filter(pl.col("country").is_in(["United Kingdom", "Japan", "Vietnam"]))
```

<br>

Do you see what could be improved here?


## 1. Optimize the code

<br>

The problem lies in the order of operations: **sorting data is much slower than filtering it**.

. . .

<br>

Let's test with a dataset of 50M observations and 10 columns:

```{r echo=FALSE}
library(dplyr, warn.conflicts = FALSE)
values <- rnorm(50 * 1e6)
data <- data.frame(
  country = rep_len(
    countrycode::codelist$country.name.en,
    length.out = 50 * 1e6
  ),
  year = sample(1960:2020, 50 * 1e6, TRUE),
  var1 = values,
  var2 = values,
  var3 = values,
  var4 = values,
  var5 = values,
  var6 = values,
  var7 = values,
  var8 = values
)
```

:::: {.columns}

::: {.column width="50%"}

```{python, eval=FALSE}
data
  .sort(pl.col("country"))
  .filter(pl.col("country").is_in(["United Kingdom", "Japan", "Vietnam"]))
```
```{r timing1, echo=FALSE, eval=TRUE, cache=TRUE}
system.time({
  data |>
    arrange(country) |>
    filter(country %in% c("United Kingdom", "Japan", "Vietnam"))
})
```

:::

::: {.column width="50%"}
```{python, eval=FALSE}
data
  .filter(pl.col("country").is_in(["United Kingdom", "Japan", "Vietnam"]))
  .sort(pl.col("country"))
```
```{r timing2, echo=FALSE, eval=TRUE, cache=TRUE}
system.time({
  data |>
    filter(country %in% c("United Kingdom", "Japan", "Vietnam")) |>
    arrange(country)
})
```

:::

::::



## 1. Optimize the query plan

<br><br>

There is probably tons of suboptimal code in our scripts.

. . .

<br>

But it's already hard enough to make scripts that work and that are
reproducible, **we don't want to spend even more time trying to
optimize them**.

. . .

<br>

{{< fa arrow-right >}} &nbsp; Let `polars` do this automatically by using lazy data.


## 1. Optimize the query plan {.nobreak}

<br>

Workflow:

1. *scan* the data to get it in *lazy* mode.

::: {.panel-tabset}

## Python

```python
import polars as pl

raw_data = pl.scan_parquet("path/to/file.parquet")

# Or
pl.scan_csv("path/to/file.csv")
pl.scan_json("path/to/file.json")
...
```
## R (polars)

```r
library(polars)

raw_data = pl$scan_parquet("path/to/file.parquet")

# Or
pl$scan_csv("path/to/file.csv")
pl$scan_json("path/to/file.json")
...
```
## R (tidypolars)

```r
library(tidypolars)

raw_data = scan_parquet_polars("path/to/file.parquet")

# Or
scan_csv_polars("path/to/file.csv")
scan_json_polars("path/to/file.json")
...
```
:::


This only returns the schema of the data: the column names and their types (character, integers, dates, ...).

## 1. Optimize the query plan {.nobreak}

<br>

Workflow:

2. Write the code that you want to run on the data: filter, sort, create new variables, etc.

::: {.panel-tabset}

## Python

```python
my_data = raw_data
   .sort("iso")
   .filter(
      pl.col("gdp") > 123,
      pl.col("country").is_in(["United Kingdom", "Japan", "Vietnam"])
   )
   .with_columns(gdp_per_cap = pl.col("gdp") / pl.col("population"))
```

## R (polars)

```r
my_data = raw_data$
   sort("iso")$
   filter(
      pl$col("gdp") > 123,
      pl$col("country")$is_in(c("United Kingdom", "Japan", "Vietnam"))
   )$
   with_columns(gdp_per_cap = pl$col("gdp") / pl$col("population"))
```

## R (tidypolars)

```r
my_data = raw_data |>
   arrange(iso) |>
   filter(
      gdp > 123,
      country %in% c("United Kingdom", "Japan", "Vietnam")
   ) |>
   mutate(gdp_per_cap =  gdp / population)
```
:::


## 1. Optimize the code {.nobreak #optimize-plan}

<br>

Workflow:

<br>

3. Call `collect()` at the end of the code to execute it.

::: {.panel-tabset}

## Python

```python
my_data.collect()
```

## R (polars)

```r
my_data$collect()
```

## R (tidypolars)

```r
my_data |> compute()
```
:::

## 1. Optimize the query plan

<br>

`polars` doesn't directly execute the code. Before that, it does a lot of optimizations to be sure that we don't do inefficient operations.

<br>

Examples of optimizations:

* do not load rows that are filtered out in the query;
* do not load variables (columns) that are never used;
* cache and reuse computations
* and many more things


## 1. Optimize the query plan

:::{.callout-tip}
You can see the "query plan" (i.e. all the steps in the code) as it was originally written with `my_data.explain(optimized=False)`.

<br>

The query plan that is **actually run by Polars** can be seen with `my_data.explain()`.

:::

# 2. Catch errors before computations

## 2. Catch errors before computations

<br><br>

Calling `collect()` doesn't start computations right away.

<br>

First, `polars` scans the code to ensure there are no schema errors, i.e. check
that we don't do "forbidden" operations.

<br>

For instance, doing `pl.col("gdp") > "France"` would be an error: we can't compare
a number to a character.

<br>

In this case, that would return:

```
polars.exceptions.ComputeError: cannot compare string with numeric data
```


# 3. Use streaming mode

## 3. Use streaming mode

<br><br>

It is possible that the collected dataset is still too big for our RAM.

<br>

In this case, this will crash the Python or R session.

<br>

. . .

**Streaming** is a way to run the code on data by batches to avoid using all memory
at the same time.

<br>

Polars takes care of splitting the data and runs the code piece by piece.



## 3. Use streaming mode

<br><br>

Using this is extremely simple: instead of calling `collect()`, we
call `collect(streaming = TRUE)`.

<br>

:::{.callout-warning}
This is still an early, incomplete feature of `polars` for now.

<br>

Not all operations can be run in streaming mode.
:::


# Q&A number 1


# Hands-on practice: UK Census data

## A note on file formats

<br>

We're used to a few file formats: CSV, Excel, `.dta`. Polars can read
most of them (`.dta` is not possible for now).

. . .

<br>

When possible, use the **Parquet format** (`.parquet`).

<br>

Pros:

* large file compression
* allows much faster filtering of rows

Cons:

* cannot be viewed in Excel (but wouldn't be possible with millions of observations anyway)


##

<br><br><br>
<br><br><br>

```{css, echo=FALSE}
#quick-demo-time {
	position: absolute;
	left: 35%;
}
```


### (Quick) Demo time!

<br>
<br>

* UK Census data

* About 40M observations, 110+ variables

# Going further

## Streaming engine

## Plugins

## GIS and Polars

While there is some demand for a `geopolars` that would enable GIS operations in Polars DataFrame or LazyFrame, this doesn't exist for now.

<br>

Groundwork for this should start in the coming months so you might expect some movement in `geopolars`in 2026.

## Going further

<br><br>

If you're interested:

* [Polars official documentation](https://docs.pola.rs/)

* [R-Polars documentation](https://pola-rs.github.io/r-polars/)

* [Python `tidypolars`](https://github.com/markfairbanks/tidypolars)

* [R `tidypolars`](https://github.com/etiennebacher/tidypolars)

* [(Pretty) big data wrangling with DuckDB and Polars](https://grantmcdermott.com/duckdb-polars/slides/slides.html)



# Conclusion

## Conclusion

<br><br>

Getting bigger computers shouldn't necessarily be the first reflex to handle
large data.

<br>

There are several other tools available: `arrow`, `DuckDB`, `Spark`, and others.

<br>

Do you use any of them? How do you deal with large datasets?

## {.nobreak}

<br><br>
<br><br>

Slides: [https://github.com/etiennebacher/handling-large-data](https://github.com/etiennebacher/handling-large-data)

<br><br>

Typos, comments: [https://github.com/etiennebacher/handling-large-data/issues](https://github.com/etiennebacher/handling-large-data/issues)
